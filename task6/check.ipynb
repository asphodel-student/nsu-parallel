{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2bN_KIsTJdyo"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import struct \n",
        "\n",
        "first_size = 32 * 32 * 16 * 16\n",
        "second_size = 16 * 16 * 4 * 4\n",
        "third_size = 4 * 4\n",
        "\n",
        "buffer_a, buffer_b, buffer_c = [], [], []\n",
        "\n",
        "with open('/content/weights1', 'rb') as f:\n",
        "  buffer_a = struct.unpack('f' * first_size, f.read(4 * first_size))\n",
        "\n",
        "with open('/content/weights2', 'rb') as f:\n",
        "  buffer_b = struct.unpack('f' * second_size, f.read(4 * second_size))\n",
        "\n",
        "with open('/content/weights3', 'rb') as f:\n",
        "  buffer_c = struct.unpack('f' * third_size, f.read(4 * third_size))\n",
        "\n",
        "with open('/content/input', 'rb') as f:\n",
        "  buffer_input = struct.unpack('f' * 1024, f.read(4 * 1024))"
      ],
      "metadata": {
        "id": "eH_ZTYvLJiub"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_a = torch.Tensor(buffer_a).reshape((16 * 16, 32 * 32))\n",
        "print(tensor_a.shape)\n",
        "\n",
        "tensor_b = torch.Tensor(buffer_b).reshape((4 * 4, 16 * 16,))\n",
        "print(tensor_b.shape)\n",
        "\n",
        "tensor_c = torch.Tensor(buffer_c).reshape((1, 4 * 4))\n",
        "print(tensor_c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INzvhEjjMExe",
        "outputId": "ac018aba-1b81-4842-d3e8-2aef1783f734"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 1024])\n",
            "torch.Size([16, 256])\n",
            "torch.Size([1, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = torch.nn.Linear(32 * 32, 16 * 16, bias=False)\n",
        "    self.fc2 = torch.nn.Linear(16 * 16, 4 * 4, bias=False)\n",
        "    self.fc3 = torch.nn.Linear(4 * 4, 1, bias=False)\n",
        "\n",
        "    self.fc1.weight = torch.nn.Parameter(tensor_a, requires_grad=True)\n",
        "    self.fc2.weight = torch.nn.Parameter(tensor_b, requires_grad=True)\n",
        "    self.fc3.weight = torch.nn.Parameter(tensor_c, requires_grad=True)\n",
        "\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = input\n",
        "    x = self.sigmoid(self.fc1(x))\n",
        "    x = self.sigmoid(self.fc2(x))\n",
        "    x = self.sigmoid(self.fc3(x))\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "o7mtnwraNVnQ"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "\n",
        "input = torch.Tensor(buffer_input).reshape((1, 1024))\n",
        "x = net(input)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG9flZvbXuXh",
        "outputId": "b19a88d3-2c58-4c24-cdb7-9ace470ff57f"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8019]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    }
  ]
}